# -*- coding: utf-8 -*-
"""Final Project: Recommendation System - Book.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GYpRDQupU8m_7czRfyFZkoajqy2wZqsQ

# **Proyek Akhir: Membuat Model Sistem Rekomendasi**

---

## Dicoding Submission
## Machine Learning Terapan

---

Kriteria submission:
- Project merupakan hasil pekerjaan sendiri.
- Project belum pernah digunakan untuk submission kelas Machine Learning di Dicoding dan belum pernah dipublikasikan di platform manapun.
- Dataset yang dipakai bebas, asal bisa digunakan untuk membuat sistem rekomendasi.
- Memberikan **dokumentasi** menggunakan **text cell** pada notebook (.ipynb) untuk menjelaskan **setiap tahapan proyek**.
- Menentukan solusi permasalahan dengan memilih pendekatan berikut:
  - Content-based Filtering
  - Collaborative Filtering
- Membuat draf laporan proyek machine learning yang menjelaskan alur proyek Anda mulai dari project overview, business understanding, data understanding, data preparation, modeling, hingga tahap evaluasi. Ketentuan draf laporan proyek machine learning dapat Anda lihat pada sub modul berikutnya tentang **Detail Laporan**.

---

Saran dan Tips:
- Menerapkan **Rubrik/Kriteria Penilaian (Tambahan)** untuk mendapatkan skala penilaian (bintang) yang lebih tinggi.
- Anda dapat memilih salah satu proyek dari domain (namun tidak terbatas pada daftar) berikut:
  - Rekomendasi film
  - Rekomendasi buku
  - Rekomendasi musik
  - Rekomendasi video
  - Rekomendasi produk 
  - Rekomendasi artikel
  - Rekomendasi berita
  - dsb.

---

- **Bintang 3** : Semua ketentuan terpenuhi, penulisan kode, dan laporan cukup baik.
- **Bintang 4** : Semua ketentuan terpenuhi, menerapkan minimal tiga (3) **Rubrik Penilaian (Tambahan)** pada laporan.
- **Bintang 5** : Semua ketentuan terpenuhi, menerapkan seluruh (6) **Rubrik Penilaian Tambahan** pada laporan.

---

# Data Diri

Nama: Andrew Benedictus Jamesie  
E-mail: andrewbjamesie@yahoo.com  

---
---

Dataset: [(Kaggle) Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset)

* [(Kaggle) Movie Recommendation System](https://www.kaggle.com/datasets/dev0914sharma/dataset)

References:

[(GitHub) Contoh Format Laporan Proyek Machine Learning](https://github.com/dicodingacademy/contoh-laporan-mlt/blob/main/format_laporan_submission_2.md)


[(GitHub) Proyek Akhir Machine Learning Terapan - Indah](https://github.com/IndahDs/dicoding-machine-learning-developer/blob/main/MLT_2/MLT_Proyek_Submission_2.ipynb)


[(Kaggle) Book Recommendation - Dicoding](https://www.kaggle.com/code/farelarden/book-recommendation-dicoding)


[(GitHub) Sistem Rekomendasi: Rekomendasi Aplikasi untuk Pengguna di Google Play Store](https://github.com/fahmij8/ML-Exercise/blob/main/MLT-2/MLT_Proyek_Submission_2.ipynb)


[(Dicoding) Kenapa Hasil Rekomendasiya Berulang?](https://www.dicoding.com/academies/319/discussions/142012)

[(Colab) RekomendasiBuah.ipynb](https://colab.research.google.com/drive/10NCdwaAFVTWNUlPC32PVYIHf-UOAhPo5)


[(Dicoding) Error saat Training pada Submission Akhir](https://www.dicoding.com/academies/319/discussions/136272)

[(Colab) Buku.ipynb](https://colab.research.google.com/drive/1FyBf5ANS3NmxXM39Mh6_yXijgYL4AYKT)

---
---

# **1. *Library Import***

*Library* [`os`](https://docs.python.org/3/library/os.html) untuk memproses *function* dari *operating system*. `os.environ` untuk membaca *username* dan *key* [Kaggle](https://kaggle.com).

*Library* [`numpy`](https://numpy.org) untuk melakukan pemrosesan matematis berupa himpunan, *array*, matriks multidimensi, dan vektorisasi.

*Library* [`pandas`](https://pandas.pydata.org) untuk melakukan pemrosesan, analisis dan manipulasi data.

*Library* [`tensorflow`](https://www.tensorflow.org) untuk melakukan pelatihan *machine learning* dan *neural networks*.

*Library* [`sklearn`](https://scikit-learn.org) untuk melakukan pemrosesan *machine learning* dan *data analysis*.

*Library* [`seaborn`](https://seaborn.pydata.org) untuk membuat visualisasi data yang berbasis `matplotlib`.

*Library* [`matplotlib`](https://matplotlib.org) untuk melakukan visualisasi menggunakan *plotting*.
"""

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import RootMeanSquaredError

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import seaborn as sns
import matplotlib.pyplot as plt

"""# **2. *Data Loading***

## 2.1 *Environment and Kaggle Credential*

Mengatur *environment* `operating system` [Colab](https://colab.research.google.com 'Google Colaboratory') dengan variabel `KAGGLE_USERNAME` dan variabel `KAGGLE_KEY` untuk menghubungkan platform [Kaggle](https://kaggle.com 'Kaggle') menggunakan [Kaggle's Beta API](https://www.kaggle.com/docs/api 'Kaggle Public API Documentation') Token.
"""

# Username dan key Kaggle API
os.environ['KAGGLE_USERNAME'] = 'andrewbjamesie'
os.environ['KAGGLE_KEY']      = '302d1e6303a6a1d0a4812c95d8ee599a'

"""## 2.2 *Dataset Download*

Mengunduh (*download*) *dataset* dari Kaggle dengan nama *file* *dataset* yang masih terkompresi (*compressed*), yaitu `book-recommendation-dataset.zip`. Berkas *dataset* tersebut digunakan dalam proyek ini adalah *dataset* [Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset 'Kaggle - Book Recommendation Dataset').
"""

# Download dataset dari Kaggle
!kaggle datasets download -d arashnic/book-recommendation-dataset

"""Melakukan ekstraksi (*extract*) berkas dataset yang masih terkompresi (*compressed*) dengan menggunakan perintah `!unzip`, sehingga didapatkan tiga (3) berkas *dataset*, yaitu `Books.csv`, `Ratings.csv`, `Users.csv` yang berupa berkas `.csv` ([comma-separated Values](https://en.wikipedia.org/wiki/Comma-separated_values 'Wikipedia - Comma-separated values'))."""

!unzip /content/book-recommendation-dataset.zip

"""# **3. *Data Understanding***

## 3.1 Jumlah Data Masing-masing Atribut dari *Dataset*

Membaca masing-masing berkas dataset, yaitu `Books.csv`, `Ratings.csv`, `Users.csv` ke dalam variabel `df_b`, `df_r`, `df_u` dengan menggunakan library [Pandas](https://pandas.pydata.org 'Python Data Analysis Library') untuk mengubahnya dari format CSV menjadi *dataframe*.
"""

df_b = pd.read_csv('Books.csv')
df_r = pd.read_csv('Ratings.csv')
df_u = pd.read_csv('Users.csv')

"""Melihat jumlah banyak data atribut yang unik pada masing-masing *dataframe* dengan menggunakan fungsi `.unique()`."""

print(f'Jumlah data ISBN     : {len(df_b["ISBN"].unique())}')
print(f'Jumlah data Judul    : {len(df_b["Book-Title"].unique())}')
print(f'Jumlah data Penulis  : {len(df_b["Book-Author"].unique())}')
print(f'Jumlah data Penerbit : {len(df_b["Publisher"].unique())}')
print(f'Jumlah data Tahun    : {len(df_b["Year-Of-Publication"].unique())}')
print(f'=====' * 9)
print(f'Jumlah data Pembaca              : {len(df_r["User-ID"].unique())}')
print(f'Jumlah data Buku                 : {len(df_r["ISBN"].unique())}')
print(f'Jumlah data Rating yang diterima : {len(df_r)}')
print(f'=====' * 9)
print(f'Jumlah data User : {len(df_u)}')

"""## 3.2 *Univariate Exploratory Data Analysis* (EDA)

*Explanatory Data Analysis* (EDA) adalah suatu proses investigasi awal pada data untuk melakukan analisis karakteristik, menemukan pola, anomali, dan memeriksa asumsi pada data dengan menggunakan bantuan statistik dan representasi grafis atau visualisasi.

### 3.2.1 *Dataset* Books

*Exploratory Data Analysis* (EDA) untuk *dataframe* `Books`.
"""

df_b

df_b.info()

"""### 3.2.2 *Dataset* Rating

*Exploratory Data Analysis* (EDA) untuk *dataframe* `Ratings`.
"""

df_r

df_r.info()

df_r.describe()

"""Deskripsi statistik untuk *dataframe* `Ratings` dengan atribut `Book-Rating`, yaitu untuk menampilkan karakteristik statistik, seperti rata-rata (`mean`), simpangan baku/standar deviasi (`std`), nilai minimum (`min`), nilai maksimum (`max`), kuartil bawah/Q1 (`25%`), kuartil tengah/Q2/median (`50%`), dan kuartil atas/Q3 (`75%`) dari *rating* pengguna terhadap buku yang sudah pernah dibaca."""

df_r['Book-Rating'].describe().apply(lambda x: '%.f' % x)

"""Visualisasi grafik histogram frekuensi sebaran data *rating* pengguna terhadap buku yang sudah pernah dibaca, mulai dari *rating* 1 hingga *rating* 10."""

df_r['Book-Rating'].value_counts().sort_index().plot(
    kind    = 'barh',
    color   = ['k', 'gray', 'pink', 'm', 'c', 'b', 'g', 'lightgreen', 'yellow', 'orange', 'r'],
    title   = 'Jumlah Rating Buku',
    xlabel  = 'Rating',
    ylabel  = 'Jumlah',
    figsize = (14, 5),
    xticks  = (np.arange(0, 720000, 50000))
).grid(linestyle='-.', linewidth=0.5)

"""Berdasarkan hasil visualisasi grafik histogram "Jumlah Rating Buku" di atas, dapat disimpulkan bahwa *rating* terbanyak dari buku yang sudah pernah dibaca adalah *rating* 0, dengan jumlah *rating* kira-kira sebanyak lebih dari 700.000. *Rating* 0 tersebut dapat menyebabkan bias dan mempengaruhi hasil analisis, sehingga data dengan *rating* 0 tersebut dapat dihapus pada tahap *data preparation*.

### 3.2.3 *Dataset* User

*Exploratory Data Analysis* (EDA) untuk *dataframe* `Users`.
"""

df_u

"""Berdasarkaan tabel *dataframe* `Users` di atas, dapat dilihat bahwa terdapat nilai `null` atau `NaN` (*Not a Number*) pada kolom/atribut `Age`. Sehingga perlu dilakukan pemrosesan lebih lanjut pada tahap *data preparation*."""

df_u.info()

"""Deskripsi statistik untuk *dataframe* `Users`, seperti jumlah data (`count`), rata-rata (`mean`), simpangan baku/standar deviasi (`std`), nilai minimum (`min`), nilai maksimum (`max`), kuartil bawah/Q1 (`25%`), kuartil tengah/Q2/median (`50%`), dan kuartil atas/Q3 (`75%`)."""

df_u.describe()

"""# **4. *Data Preprocessing***

Tahap pra-pemrosesan data atau *data preprocessing* merupakan tahap yang perlu diterapkan sebelum melakukan proses pemodelan. Tahap ini adalah teknik yang digunakan untuk mengubah data mentah *(raw data*) menjadi data yang bersih (*clean data*) yang siap untuk digunakan pada proses selanjutnya. Dalam kasus ini, tahap *data preprocessing* dilakukan dengan menyesuaikan nama kolom atau atribut masing-masing *dataframe*, melakukan penggabungkan data ISBN, dan data *User* untuk melihat jumlah data secara keseluruhan.

## 4.1 Mengubah Nama Kolom/Atribut

Tujuan dari proses pengubahan nama kolom atau artibut dari masing-masing *dataframe* dengan menggunakan fungsi `.rename()` adalah untuk memudahkan proses pemanggilan *dataframe* dengan nama kolom atau atribut tertentu pada tahap selanjutnya.

### 4.1.1 Books
"""

df_b.rename(columns={
    'ISBN'                : 'isbn',
    'Book-Title'          : 'book_title',
    'Book-Author'         : 'book_author',
    'Year-Of-Publication' : 'pub_year',
    'Publisher'           : 'publisher',
    'Image-URL-S'         : 'image_s_url',
    'Image-URL-M'         : 'image_m_url',
    'Image-URL-L'         : 'image_l_url'
}, inplace=True)

df_b

"""### 4.1.2 Ratings"""

df_r.rename(columns={
    'User-ID'     : 'user_id',
    'ISBN'        : 'isbn',
    'Book-Rating' : 'book_rating'
}, inplace=True)

df_r

"""### 4.1.3 Users"""

df_u.rename(columns={
    'User-ID'  : 'user_id',
    'Location' : 'location',
    'Age'      : 'age'
}, inplace=True)

df_u

"""## 4.2 Menggabungkan Data ISBN

Penggabungan data ISBN buku dilakukan menggunakan fungsi `.concatenate` dengan bantuan *library* [`numpy`](https://numpy.org). Data ISBN terdapat pada *dataframe* buku (`df_b`) dan *dataframe* *rating* (`df_r`), sehingga dilakukan penggabungan data tersebut pada atribut atau kolom `isbn`.
"""

ISBNAll = np.concatenate((
    df_b.isbn.unique(),
    df_r.isbn.unique()
))

ISBNAll = np.sort(np.unique(ISBNAll))

print(f'Jumlah Buku berdasarkan ISBN : {len(ISBNAll)}')

"""## 4.3 Menggabungkan Data *User*

Penggabungan data `user_id` buku dilakukan menggunakan fungsi `.concatenate` dengan bantuan *library* [`numpy`](https://numpy.org). Data `user_id` terdapat pada *dataframe* *rating* (`df_r`) dan *dataframe* *user* (`df_u`), sehingga dilakukan penggabungan data tersebut pada atribut atau kolom `user_id`.
"""

UserAll = np.concatenate((
    df_r.user_id.unique(),
    df_u.user_id.unique()
))

UserAll = np.sort(np.unique(UserAll))

print(f'Jumlah Buku berdasarkan ISBN : {len(UserAll)}')

"""# **5. *Data Preparation***

Tahap persiapan data atau *data preparation* juga merupakan tahapan penting sebelum memasuki proses pengembangan model machine learning. Pada tahap ini, dilakukan proses transformasi pada data sehingga menjadi bentuk yang cocok untuk proses pemodelan nantinya. Dalam kasus ini, tahap *data preparation* dilakukan dengan mengatasi *missing value*, pengecekan data duplikat, dan penggabungan data buku dan data *rating*.

## 5.1 *Pengecekan* *Missing Value*

Proses pengecekan missing value pada *dataframe* dapat dilakukan dengan menggunakan fungsi `.isnull().sum()`, sehingga diperoleh total jumlah data yang kosong atau hilang (*missing*).

### 5.1.1 Books
"""

books = df_b
books.isnull().sum()

"""Berdasarkan deskripsi di atas, dapat dilihat bahwa pada *dataframe* `books` terdapat atribut yang memiliki nilai kosong atau *null*, yaitu pada atribut `book_author` sebanyak 1 data, `publisher` sebanayk 2 data, dan `image_l_url` sebanyak 3 data.

Dengan begitu, data yang kosong tersebut dapat dihapus atau di-*drop* dengan menggunakan fungsi `.dropna()`. Sehingga jika dilakukan pengecekan kembali maka sudah tidak ditemukan data yang kosong atau *null*.
"""

books = books.dropna()
books.isnull().sum()

"""### 5.1.2 Ratings"""

ratings = df_r
ratings.isnull().sum()

"""Berdasarkan deskripsi di atas, dapat dilihat bahwa pada *dataframe* `ratings` tidak ditemukan adanya nilai kosong atau *null* di setiap atribut atau kolomnya.

Namun, pada tahap *Univariate Exploratory Data Analysis* (EDA) sebelumnya, dapat dilihat bahwa pada hasil visualisasi grafik histogram "Jumlah Rating Buku", sebagian besar data *rating* dari buku yang sudah pernah dibaca oleh *user* terdapat pada *rating* 0, dengan jumlah *rating* kira-kira sebanyak lebih dari 700.000. *Rating* tersebut dapat menyebabkan bias pada analisis data, sehingga data dengan *rating* 0 dapat dihapus.
"""

print(f'Total Rating 0 : {ratings.book_rating.eq(0).sum()}')

ratings = ratings[df_r.book_rating > 0]

"""Data dengan *rating* 0 ternyata sebanyak 716.109 data. Data tersebut tidak akan diikutsertakan ke dalam *dataframe*, sehingga data yang diambil adalah data *rating* yang lebih dari 0, yaitu *rating* 1 hingga *rating* 10 saja."""

ratings.book_rating.value_counts().sort_index().plot(
    kind    = 'barh',
    color   = ['gray', 'pink', 'm', 'c', 'b', 'g', 'lightgreen', 'yellow', 'orange', 'r'],
    title   = 'Jumlah Rating Buku',
    xlabel  = 'Rating',
    ylabel  = 'Jumlah',
    figsize = (14, 5),
    xticks  = (np.arange(0, 110000, 7500))
).grid(linestyle='-.', linewidth=0.5)

"""Berdasarkan hasil visualisasi grafik histogram di atas dengan *rating* 0 yang telah dihapus, dapat dilihat distribusi frekuensi data yang lebih rapi dan jelas, terutama pada data *rating* 1 hingga *rating* 4.

### 5.1.3 Users
"""

users = df_u
users.isnull().sum()

"""Berdasarkan deskripsi di atas, dapat dilihat bahwa pada *dataframe* `users` terdapat atribut yang memiliki nilai kosong atau *null*, yaitu pada atribut `age` sebanyak 110.762 data.

Dengan begitu, data yang kosong tersebut dapat diganti atau diisi dengan nilai modus atau nilai yang paling sering muncul dalam data `age` tersebut dengan menggunakan fungsi `.fillna()` dan fungsi ` .mode()`.
"""

users.age = users.age.fillna(users.age.mode())
users.isnull().sum()

users.age.hist(bins=100)

"""Berdasarkan hasil visualisasi grafik histogram umur *user* di atas dapat dilihat bahwa rentang umur *user* paling banyak berada pada umur 20 hingga 30-an.

## 5.2 Pengecekan Data Duplikat

Melakukan pengecekan data yang duplikat atau data yang sama pada *dataframe* dapat dilakukan dengan menggunakan fungsi `.duplicated().sum()`.
"""

print(f'Jumlah data books  yang duplikat: {books.duplicated().sum()}')
print(f'Jumlah data rating yang duplikat: {ratings.duplicated().sum()}')
print(f'Jumlah data users  yang duplikat: {users.duplicated().sum()}')

"""Berdasarkan data di atas, dapat dilihat bahwa tidak terdapat data buku, *rating*, dan *user* yang sama atau duplikat.

## 5.3 Data Buku dan *Rating*
"""

books_ratings = pd.merge(ratings, books, on=['isbn'])
books_ratings

"""# **6. *Modeling***

Tahap selanjutnya adalah proses *modeling* atau membuat model *machine learning* yang dapat digunakan sebagai sistem rekomendasi untuk menentukan rekomendasi buku yang terbaik kepada pengguna dengan beberapa algoritma sistem rekomendasi tertentu.

Berdasarkan tahap pemahaman data atau data understanding sebelumnya, dapat dilihat bahwa data untuk masing-masing *dataframe*, yaitu data buku, *rating*, dan *users* tergolong data yang cukup banyak, mencapai ratusan hingga jutaan data. Hal tersebut akan berdampak pada biaya yang akan diperlukan untuk melakukan proses pemodelan *machine learning*, seperti memakan waktu yang lama dan *resource* RAM ataupu GPU yang cukup besar. Oleh karena itu, dalam kasus ini data yang akan digunakan untuk proses pemodelan *machine learning* data akan dibatasi hanya 10.000 baris data buku dan 5000 baris data *rating*.
"""

books   = books[:10000]
ratings = ratings[:5000]

"""## 6.1 *Content-based Recommendation*

Sistem rekomendasi yang berbasis konten (*Content-based Recommendation*) adalah sistem rekomendasi yang merekomendasikan item yang mirip dengan item yang disukai pengguna di masa lalu. *Content-based filtering* akan mempelajari profil minat pengguna baru berdasarkan data dari objek yang telah dinilai pengguna.

### 6.1.1 TF-IDF Vectorizer

*Term Frequency Inverse Document Frequency Vectorizer* ([TF-IDF Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html 'TfidfVectorizer - scikit-learn Documentation')) *Algorithm* merupakan algoritma yang dapat melakukan kalkulasi dan transformasi dari teks mentah menjadi representasi angka yang memiliki makna tertentu dalam bentuk matriks serta dapat digunakan dan dimengerti oleh model *machine learning*.
"""

tfidf = TfidfVectorizer()
tfidf.fit(books.book_author)
# tfidf.get_feature_names_out()

"""Proses transformasi data buku dengan atribut `book_author` ke dalam bentuk matriks dapat dilakukan dengan menggunakan fungsi `.fit_transform()`."""

tfidf_matrix = tfidf.fit_transform(books.book_author)
tfidf_matrix.shape

"""Ukuran matriks yang dihasilkan dari transformasi tersebut adalah 10.000 data buku dan 5.575 data *author*.

Data di atas masih dalam bentuk vektor (dari *vectorizer*), sehingga perlu diubah ke dalam bentuk matriks dengan menggunakan fungsi `.todense()`.
"""

tfidf_matrix.todense()

"""Untuk dapat melihat matriks TF-IDF, matriks tersebut diubah terlebih dahulu menjadi sebuah *dataframe* dengan kolom adalah nama *author*, dan baris (*index*) merupakan judul buku."""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns = tfidf.get_feature_names_out(),
    index   = books.book_title
).sample(20, axis=1).sample(10, axis=0)

"""### 6.1.2 *Cosine Similarity*

Untuk melakukan perhitungan derajat kesamaan (*similarity degree*) antar judul buku dapat dilakukan dengan teknik *cosine similarity* menggunakan fungsi [`cosine_similarity`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html 'cosine_similarity - scikit-learn Documentation') dari library `sklearn`.
"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Dengan teknik yang sama, untuk melihat *array* *cosine similarity* dapat diubah terlebih dahulu menjadi sebuah *dataframe*."""

cosine_sim_df = pd.DataFrame(
    cosine_sim,
    columns = books.book_title,
    index   = books.book_title
)

print(f'Cosine Similarity Shape : {cosine_sim_df.shape}')

cosine_sim_df.sample(8, axis=1).sample(8, axis=0)

"""### 6.1.3 Recommendation Testing

Mendefinisikan fungsi `author_recommendations` untuk menampilkan data buku yang direkomendasikan oleh algoritma sistem yang telah dibuat, dengan parameter masukan berupa judul buku yang sudah pernah dibaca oleh *user*.
"""

def author_recommendations(book_title, similarity_data=cosine_sim_df, items=books[['book_title', 'book_author']], k=10):
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(book_title, errors='ignore')
    
    return pd.DataFrame(closest).merge(items).head(k)

readed_book_title = 'Proxies'

books[books.book_title.eq(readed_book_title)]

"""Pada beberapa kasus, sistem rekomendasi akan memberikan rekomendasi buku yang terduplikat, sehingga perlu dilakukan penghapusan data judul buku rekomendasi yang terduplikat."""

author_recommendations(readed_book_title).drop_duplicates()

"""Dapat dilihat bahwa sistem yang telah dibangun berhasil memberikan rekomendasi beberapa judul buku berdasarkan input atau masukan sebuah judul buku, yaitu "Proxies", dan diperoleh beberapa judul buku yang berdasarkan perhitungan sistem.

## 6.2 *Collaborative Filtering Recommendation*

Sistem rekomendasi penyaringan kolaboratif (*Collaborative Filtering Recommendation*) adalah sistem rekomendasi yang merekomendasikan item yang mirip dengan preferensi pengguna di masa lalu, misalnya berdasarkan *rating* yang telah diberikan oleh pengguna di masa lalu.

### 6.2.1 *Data Preparation*

Melakukan penyandian (*encoding*) fitur `user_id` ke dalam indeks integer.
"""

user_ids = ratings.user_id.unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

print(user_ids)
print(user_to_user_encoded)
print(user_encoded_to_user)

"""Melakukan penyandian (*encoding*) fitur `isbn` buku ke dalam indeks integer."""

book_ids = ratings.isbn.unique().tolist()
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

print(book_ids)
print(book_to_book_encoded)
print(book_encoded_to_book)

"""Memetakan `user_id` dan `isbn` ke dalam masing-masing *dataframe* yang berkaitan."""

ratings['user'] = ratings.user_id.map(user_to_user_encoded)
ratings['book'] = ratings.isbn.map(book_to_book_encoded)

"""Melakukan pengecekan jumlah *user*, jumlah buku, dan *rating* minimal serta *rating* maksimal."""

num_users = len(user_encoded_to_user)
num_books = len(book_encoded_to_book)
print(num_users)
print(num_books)

min_ratings = min(ratings.book_rating)
max_ratings = max(ratings.book_rating)
print(f'Number of User: {num_users}, Number of Books: {num_books}, Min Rating: {min_ratings}, Max Rating: {max_ratings}')

"""### 6.2.2 *Training Data and Validation Data Split*

Melakukan pengecekan terdahap *dataframe* `ratings` yang telah dilakukan pemetaan atribut atau kolom tambahan, yaitu `user` dan `book`. Selain itu, dilakukan juga pengacakan data dengan menggunakan fungsi [`.sample(frac=1)`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html 'pandas.DataFrame.sample - Pandas Documentation').
"""

ratings = ratings.sample(frac=1, random_state=412)
ratings

"""Melakukan pembagian *dataset* dengan rasio 80:20, yaitu 80% untuk data latih (*training data*) dan 20% untuk data uji (*validation data*)."""

x = ratings[['user', 'book']].values
y = ratings['book_rating'].apply(lambda x: (x-min_ratings) / (max_ratings-min_ratings)).values

train_indices = int(0.8 * ratings.shape[0])

xTrain, xVal, yTrain, yVal = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""### 6.2.3 *Model Development and Training*

Pada tahap pembuatan model akan menggunakan kelas `RecommenderNet` dengan [*keras model class*](https://keras.io/api/models/model 'Model class - Keras Documentation').
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_books, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_books = num_books
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.user_bias      = layers.Embedding(num_users, 1)
        self.book_embedding = layers.Embedding(
            num_books,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.book_bias = layers.Embedding(num_books, 1)
    
    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:,0])
        user_bias   = self.user_bias(inputs[:, 0])
        book_vector = self.book_embedding(inputs[:, 1])
        book_bias   = self.book_bias(inputs[:, 1])
        
        dot_user_book = tf.tensordot(user_vector, book_vector, 2) 
        
        x = dot_user_book + user_bias + book_bias
        
        return tf.nn.sigmoid(x)

"""Kemudian pada proses *model compiling*, akan menggunakan [Adam optimizer](https://keras.io/api/optimizers/adam 'Adam - Keras Documentation'), [binary crossentropy loss function](https://keras.io/api/losses/probabilistic_losses/#binarycrossentropy-class 'BinaryCrossentropy - Keras Documentaion'), dan metrik [RMSE](https://keras.io/api/metrics/regression_metrics/#rootmeansquarederror-class 'RootMeanSquaredError - Keras Documentation') (Root Mean Squared Error)."""

model = RecommenderNet(num_users, num_books, 50)

model.compile(
    optimizer = Adam(learning_rate=0.001),
    loss      = BinaryCrossentropy(),
    metrics   = [RootMeanSquaredError()]
)

"""Pelatihan model atau model *training* dengan menggunakan fungsi `.fit()` dengan parameter `batch_size` sebesar 20, dan 30 `epochs`."""

history = model.fit(
    x               = xTrain,
    y               = yTrain,
    batch_size      = 20,
    epochs          = 30,
    validation_data = (xVal, yVal),
)

"""Melakukan visualisasi hasil *training* dan *validation* *error* serta *training* dan *validation* *loss* ke dalam grafik plot dengan bantuan *library* [`matplotlib`](https://matplotlib.org 'Matplotlib - Visualization with Python')."""

rmse     = history.history['root_mean_squared_error']
val_rmse = history.history['val_root_mean_squared_error']

loss     = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize = (12, 4))
plt.subplot(1, 2, 1)
plt.plot(rmse,     label='RMSE')
plt.plot(val_rmse, label='Validation RMSE')
plt.title('Training and Validation Error')
plt.xlabel('Epoch')
plt.ylabel('Root Mean Squared Error')
plt.legend(loc='lower right')

plt.subplot(1, 2, 2)
plt.plot(loss,     label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

plt.show()

"""## 6.2.4 *Get Recommendation Testing*

Melakukan pendefinisian ulang *dataset* *books* dan *ratings*.
"""

datasetBook   = books
datasetRating = ratings

"""Untuk mendapatkan rekomendasi buku yang akan dihasilkan oleh sistem, diperlukan sebuah data atau sampel dari pengguna secara acak dan mendefinisikan variabel buku yang belum pernah dibaca oleh pengguna (`notReadedBooks`) yang merupakan daftar resto yang nantinya akan direkomendasikan. Daftar tersebut dapat didapatkan dengan menggunakan operator logika bitwise ([`~`](https://docs.python.org/3/reference/expressions.html#unary-arithmetic-and-bitwise-operations 'Unary Arithmetic and Bitwise Operations - Python Documentation')) pada variabel buku yang telah dibaca oleh pengguna (`readedBooks`)."""

userId      = datasetRating.user_id.sample(1).iloc[0]
readedBooks = datasetRating[datasetRating.user_id == userId]

notReadedBooks = datasetBook[~datasetBook['isbn'].isin(readedBooks.isbn.values)]['isbn'] 
notReadedBooks = list(
    set(notReadedBooks).intersection(set(book_to_book_encoded.keys()))
)

notReadedBooks = [[book_to_book_encoded.get(x)] for x in notReadedBooks]
userEncoder    = user_to_user_encoded.get(userId)
userBookArray = np.hstack(
    ([[userEncoder]] * len(notReadedBooks), notReadedBooks)
)

"""Untuk mendapatkan hasil rekomendasi buku yang akan diberikan oleh sistem, dapat menggunakan fungsi [`.predict()`](https://keras.io/api/models/model 'Model class - Keras Documentation') dari *library* Keras."""

ratings = model.predict(userBookArray).flatten()

topRatingsIndices   = ratings.argsort()[-10:][::-1]
recommendedBookIds = [
    book_encoded_to_book.get(notReadedBooks[x][0]) for x in topRatingsIndices
]

print('Showing recommendations for users: {}'.format(userId))
print('=====' * 8)
print('Book with high ratings from user')
print('-----' * 8)

topBookUser = (
    readedBooks.sort_values(
        by = 'book_rating',
        ascending=False
    )
    .head(5)
    .isbn.values
)

bookDfRows = datasetBook[datasetBook['isbn'].isin(topBookUser)]
for row in bookDfRows.itertuples():
    print(row.book_title, ':', row.book_author)

print('=====' * 8)
print('Top 10 Books Recommendation')
print('-----' * 8)

recommended_resto = datasetBook[datasetBook['isbn'].isin(recommendedBookIds)]
for row in recommended_resto.itertuples():
    print(row.book_title, ':', row.book_author)

"""Berdasarkan hasil di atas, dapat dilihat bahwa sistem akan mengambil pengguna secara acak, yaitu pengguna dengan `user_id` **388**. Lalu akan dicari buku dengan rating terbaik dari user tersebut, yaitu,
*   **Impossible Vacation** oleh **Connie Willis**
*   **Life Before Man** oleh **Margaret Atwood**
*   **Cavedweller** oleh **Dorothy Allison**
*   **The Robber Bride** oleh **Margaret Atwood**
*   **The House of Mirth (Library of America)** oleh **Edith Wharton**

Kemudian sistem akan membandingan antara buku dengan rating tertinggi dari user dan semua buku, kecuali buku yang telah dibaca tersebut, lalu akan mengurutkan buku yang akan direkomendasikan berdasarkan nilai rekomendasi yang tertinggi. Dapat dilihat terdapat 10 daftar buku yang direkomendasikan oleh sistem.

Dapat dibandingkan antara ***Book with high ratings from user*** dan ***Top 10 Books Recommendation***, terdapat buku dengan penulis atau author yang sama, yaitu **The Handmaid's Tale** oleh **Margaret Atwood**. Dengan begitu, dapat dikatakan bahwa sistem yang telah dibangun dapat merekomendasikan buku kepada pengguna dengan prediksi yang cukup sesuai.

# **7. Kesimpulan**

Kesimpulannya adalah model yang digunakan untuk melakukan rekomendasi buku berdasarkan teknik *Content-based Recommendation* dan teknik *Collaborative Filtering Recommendation* telah berhasil dibuat dan sesuai dengan preferensi pengguna. Pada *collaborative filtering* diperlukan data rating dari pengguna, sedangkan pada *content-based filtering*, data rating tidak diperlukan karena analisis sistem rekomendasi akan berdasarkan atribut item dari masing-masing buku.
"""